var documenterSearchIndex = {"docs":
[{"location":"Documentation/#Documentation","page":"Documentation","title":"Documentation","text":"","category":"section"},{"location":"Documentation/","page":"Documentation","title":"Documentation","text":"Documentation for JuliaGo.jl","category":"page"},{"location":"Documentation/","page":"Documentation","title":"Documentation","text":"Pages = [\"Documentation.md\"]","category":"page"},{"location":"Documentation/#Types-and-Functions","page":"Documentation","title":"Types & Functions","text":"","category":"section"},{"location":"Documentation/#MCTS","page":"Documentation","title":"MCTS","text":"","category":"section"},{"location":"Documentation/","page":"Documentation","title":"Documentation","text":"Modules = [JuliaGo]\nPages = [\"src/MCTS.jl\"]","category":"page"},{"location":"Documentation/#JuliaGo.Entry","page":"Documentation","title":"JuliaGo.Entry","text":"Entry\n\nType storing the wins, draws and number of visits for a Node.\n\n\n\n\n\n","category":"type"},{"location":"Documentation/#JuliaGo.Node","page":"Documentation","title":"JuliaGo.Node","text":"Node\n\nType used to represent nodes on the game tree. \n\n\n\n\n\n","category":"type"},{"location":"Documentation/#JuliaGo.MCTS!-Tuple{Chess.Board, Node, UInt64, Any}","page":"Documentation","title":"JuliaGo.MCTS!","text":"MCTS!(b::Board, P::Node, num_rollouts::UInt64)\n\nMonte Carle Tree Search algorithm. Returns the Node with the highest visit count via robust_child.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.backpropagate!-Tuple{Node, Chess.Board, Dict{Chess.PieceColor, UInt64}}","page":"Documentation","title":"JuliaGo.backpropagate!","text":"backpropagate!(C::Node, b::Board, results::Dict{PieceColor,UInt64})\n\nBackpropagates the result of a rollout up the tree until the root node. The root node is defined by root.parent == nothing. Returns the root node.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.expansion!-Tuple{Chess.Board, Node, UInt64, Any}","page":"Documentation","title":"JuliaGo.expansion!","text":"expansion!(b::Board, P::Node, num_rollouts::UInt64)::Node\n\nSelects a child node at random and removes the move from the parent node's unexplored_moves list. Conducts rollouts from the child node until a terminal state and backpropagates the result until the root node of of the tree (P is not the root node). This rollout function is multi-threaded i.e. each rollout is perfomed by a different thread. The results are collected and counted (making it all thread safe).\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.firstpass!-Tuple{Chess.Board, Node, UInt64}","page":"Documentation","title":"JuliaGo.firstpass!","text":" firstpass!(b::Board, P::Node, num_rollouts::UInt64)\n\nGiven the root node of tree, where no simulation have yet occured, populate all the children of the root node with a value based on rollouts and back propagation.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.robust_child-Tuple{Node}","page":"Documentation","title":"JuliaGo.robust_child","text":"robust_child(P::Node)::Move\n\nSelects the child with the highest visit count N and returns the Move associated with that child. This is the default method of choosing the action but others exist, see secure child and max child.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.rollout-Tuple{Chess.Board, Any}","page":"Documentation","title":"JuliaGo.rollout","text":"rollout(b::Board, policy)::PieceColor\n\nPlays moves according to a policy, a neural network, until a terminal state is reached. When a terminal state is reached the colour of the winning side is returned, COLOR_NONE if it is a draw\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.rollout-Tuple{Chess.Board}","page":"Documentation","title":"JuliaGo.rollout","text":"rollout(b::Board)::PieceColor\n\nPlays moves randomly until a terminal state is reached.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.selection_policy-Tuple{Node}","page":"Documentation","title":"JuliaGo.selection_policy","text":"function selection_policy(C::Node)\n\nCalculates the Upper Confidence Bound for Trees (UCT) via\n\nmathrmUCT = fracw_in_i + Csqrtfracln N_in_i\n\nwhere\n\nw_i is the number of wins associated with taking the action that results in this node i.e. the number of wins from the parent's perspecive, n_i is the number of simulations from this node, C = sqrt2 is a coefficient controlling the balance between exploration and exploitation, N_i is the number of simulations of the parent node.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#Neural-Network","page":"Documentation","title":"Neural Network","text":"","category":"section"},{"location":"Documentation/","page":"Documentation","title":"Documentation","text":"Modules = [JuliaGo]\nPages = [\"src/policy/input.jl\", \"src/policy/ResNetBlock.jl\", \"src/policy/policy.jl\"]","category":"page"},{"location":"Documentation/#JuliaGo.nn_input-Tuple{Chess.Board}","page":"Documentation","title":"JuliaGo.nn_input","text":"nn_input\n\nConvert a board to a (8, 8, 17) Array{Float32}, suitable for input to the neural network. There are 12 (8, 8) arrays for the pieces, 4 for the castling rights and 1 for the colour of the side to move.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.nn_output-Tuple{Chess.Board, Chess.Board}","page":"Documentation","title":"JuliaGo.nn_output","text":"nn_output(b::Board, next_board::Board)::Array{Float32,3}\n\nGiven a board b and the next_board, convert the move to a (8, 8, 73) array. The 73 values are encoded as follows: 1-7 horizontal right, 8-14 horizontal left, 15-21 vertical up, 22-28 vertical down, 29-35 north east, 36-42 north west, 42-49 south east, 50-56 south west, 57-64 = knight moves, 65-67 pawn mv NE and underpromote to bishop, knight, rook, 68-70 pawn mv NW, 71-73 pawn mv N.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.output_to_move-Tuple{Any}","page":"Documentation","title":"JuliaGo.output_to_move","text":"output_to_move\n\nConvert the index of the highest probability element in the model output array to a move.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.parse_pgns-Tuple{Any, Any}","page":"Documentation","title":"JuliaGo.parse_pgns","text":"parse_pgns\n\nParse a list of pgns and return arrays for training the neural network. Positions in each game are selected according with probability prob to prevent highly correlated positions.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.predict_move-Tuple{Chess.Board, Any}","page":"Documentation","title":"JuliaGo.predict_move","text":"predict_move\n\nGiven a board and policy, output the array of possible next moves with associated probabilty. If the policy outputs and illegal move, a legal move is selected at random.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.train_test_data-Tuple{Any, Any, Any}","page":"Documentation","title":"JuliaGo.train_test_data","text":"input(preloaded, ratio)\n\nCreate train and test sets from the pgns, split according to the ratio.\n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.ResNetBlock-Tuple{}","page":"Documentation","title":"JuliaGo.ResNetBlock","text":"ResNetBlock()\n\nDefines a single ResNet block - two rectified batch-normalized convolutional layers with a skip connection. \n\n\n\n\n\n","category":"method"},{"location":"Documentation/#JuliaGo.batch_train!","page":"Documentation","title":"JuliaGo.batch_train!","text":"batch_train!(loss,\nps,\ntrain_x,\ntrain_y,\ntest_x,\ntest_y,\nopt,\nmodel,\nbatchsize = 64,\nepochs = 100000,)\n\nAt each epoch, a random selection of the train_x data of size batchsize is trasferred to the gpu and the model trained - this is mini-batch gradient descent. The model is saved in cpu format at every 5% of the total number of epochs.\n\n\n\n\n\n","category":"function"},{"location":"Documentation/#JuliaGo.policy_network_train","page":"Documentation","title":"JuliaGo.policy_network_train","text":"policy_network_train(preloaded_data=false, model_name=\"policy_network.bson\")\n\nTrain a policy network on a dataset of professional chess games. The aim of the policy network is to predict the move from a state that a professional would likely play.\n\nBy default the model is transferred to the gpu.\n\nPass preloaded_data=true if the pgns have already been parsed and converted into a input for the neural network (considerable time saving).\n\n\n\n\n\n","category":"function"},{"location":"Documentation/#Index","page":"Documentation","title":"Index","text":"","category":"section"},{"location":"Documentation/","page":"Documentation","title":"Documentation","text":"","category":"page"},{"location":"#JuliaGo.jl","page":"Home","title":"JuliaGo.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Github: github.com/chris-haggard/JuliaGo.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"An Julia chess engine based on MCTS and neural networks, similar to AlphaGo. This is a hobby project to further my understanding of artificial intelligence in the context of games and the Julia programming language. This began as an attempt to implement the full AlphaGo paper. This combines MCTS, neural networks and reinforcement learning. I only tackle the first two due to resource constraints (lack of GPUs) and time!","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"index.md\"]","category":"page"},{"location":"#JuliaGo.jl-Documentation","page":"Home","title":"JuliaGo.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Get started with the Documentation.","category":"page"},{"location":"#Artificial-Intelligence-and-Games","page":"Home","title":"Artificial Intelligence and Games","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Two player games, such as Chess and Go, are often used as a testing ground for artificial intelligence algorithms. They provide a well-bounded problem, with obvious rules and a clear winner. Chess is a two-player zero-sum (an advantage for one side is an equivalent disadvantage for the other) perfect-information game and is the target of this work. A game of chess can be represented as a tree, referred to as the 'game-tree'. This game-tree consists of nodes representing states in the game. The game-tree in chess has a branching factor, the average number of child nodes per parent, of 31-35. In the Go the branching factor is 200-250. The number of number of nodes grows exponentially, rendering a brute force approach impossible and so we turn to artificial intelligence algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Go game-tree) The Go game-tree","category":"page"},{"location":"#Computer-Chess","page":"Home","title":"Computer Chess","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For a given chess board S_0, the aim of any chess engine (or human), is to find the optimal action A_i from a set of actions A_i in A_1ldotsA_N that result in the next possible boards (states) S_1 ldots S_N. The optimal action is that which results, after many such actions, in a win for the player. The optimal value function nu^*(s) is strictly 0 for non-terminal states and, in terminal states, +1 0 -1 for a win, draw and loss respectively. A brute-force approach would require recursively finding nu^*(s) for every action/state in a game-tree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The standard algorithm for computer chess in minimax. In minimax the game-tree is recursively explored by depth-first search (DFS) until a given predecided depth. At each state the action is taken to minimise the opponent’s maximum reward. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"nu_i = max_a_i min_a_-i(nu_i(a_i a_-i))","category":"page"},{"location":"","page":"Home","title":"Home","text":"where i rightarrow -i indicates the switching of players. At the predecided maximum depth, which is very unlikely to be a terminal state, the remaining sub-tree is discarded and instead nu^*(s) is approximated, nu(s) approx nu^*(s). nu(s), often called a static evaluation function, assigns a score to the state. This score reflects the likely outcome from the state and is necessarily heuristic, a function of the number of pieces and position, as a chess game is only decided by it's value in the terminal state (checkmate or draw). Good static evaluation functions are complex to create. The breadth of the game-tree is typically pruned with alpha-beta pruning.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Chess minimax) Minimax algorithm for chess. At each state the action is selected that minimises the opponents maximum score. Only the terminal nodes are passed to the static evaluation function.","category":"page"},{"location":"#Monte-Carlo-Tree-Search","page":"Home","title":"Monte Carlo Tree Search","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Monte Carlo Tree Search (MCTS) offers an alternative methodology to an evaluation function. Instead of terminating the search and evaluating the position heuristically, MCTS evaluates the actions available from a position by conducting rollouts - searches to terminal without any branching. Taking the average of many rollouts provides an approximation to the value of the given action. This is formulated as (following Browne et al.[1])","category":"page"},{"location":"","page":"Home","title":"Home","text":"Q(s a) = frac1N(sa)sum_i=1^N(s)mathcalI_i(sa)z_i","category":"page"},{"location":"","page":"Home","title":"Home","text":"where","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nQ(s a) text - the value of taking action a from state s\nN(s a) text - the number of times a is selected from s\nN(s) text - the total number of simulations from s\nz_i text - the result of the i-th playout +1 0 -1\nmathcalI_i(s a) text - is 1 if a was selected from s on the i-th playout\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"These values that are assigned to a specific action are then used to guide the search to best-first strategy. An asymmetric game-tree is progressively built - at each iteration the tree is guided by the previous search. The value associate with the most promising moves should become more accurate as the tree is built as they are preferentially searched in further depth as the tree grows. The key to MCTS is finding the balance between exploitation and exploration: exploiting the current most promising moves and exploring those that currently look less promising but could turn out to be more optimal given further exploration.","category":"page"},{"location":"#MCTS-algorithm","page":"Home","title":"MCTS algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: MCTS algorithm)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The MCTS algorithm consists of 4 steps:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Selection: from S a selection policy that biases the search is recursively applied to child nodes until a non-fully expanded node is reached.\nExpansion: a child node is added to the tree.\nSimulation: a number of rollouts are played according to a default policy (a policy of selecting the next action) until a terminal state is reached\nBackpropagation: the results of the rollouts are propagated up the entire tree.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The MCTS can be terminated at anytime and a is selected according to a policy.","category":"page"},{"location":"#Biasing-the-search:-the-selection-policy","page":"Home","title":"Biasing the search: the selection policy","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The selection stage of the MCTS relies on a selection policy. The simplest policy would be to select child nodes from a uniform distribution. A more efficient method would bias the selection towards more promising actions. However, at the same time as exploiting promising actions, the selection policy must not totally discount less promising actions as these might be more favourable if the tree was explored in more depth. This is the key to exploration-exploitation. The most common selection policy is the Upper Confidence Bound for Trees (UCT):","category":"page"},{"location":"","page":"Home","title":"Home","text":"mathrmUCT = fracw_in_i + Csqrtfracln N_in_i","category":"page"},{"location":"","page":"Home","title":"Home","text":"where","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nw_i text - is the number of wins associated with taking the action that results in this node ie the number of wins from the parents perspecive\nn_i text - is the number of simulations from this node\nC = sqrt2 text - is a coefficient controlling the balance between exploration and exploitation\nN_i text - is the number of simulations of the parent node\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"The child node with the maximum value of UCT is selected. The first term in UCT encourages the exploitation of promising actions and the second term biases towards less explored nodes. The selection policy results in asymmetric tree growth.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Asymmetric tree growth) The game-tree grows asymmetrically due to the selection policy.","category":"page"},{"location":"#Selecting-an-action","page":"Home","title":"Selecting an action","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"At the end of MCTS, we are left with a set of actions A_1ldotsA_N each with an associated value Q. The standard method for selecting an action is the robust child method. This involves selecting a_i associated with creating s_i that has the highest number of visits (not the highest score i.e wins).","category":"page"},{"location":"#Default-Policy:-training-a-neural-network-to-predict-moves","page":"Home","title":"Default Policy: training a neural network to predict moves","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MCTS relies on a default policy to select moves in the rollouts. Randomly selecting moves would be a poor policy; it would require a great many rollouts to converge the value of the action. Instead we require a policy that selects the optimal action. This is impossible; if we knew the optimal action we would not be conducting MCTS. We approximate the optimal action with the action that a professional chess player would play. To extract this we need come supervised machine learning, for this we use a neural network (NN). The data set is a set of professional chess games. The input to the NN is the state and the output is the a probability distribution of legal moves.","category":"page"},{"location":"#Neural-network-architecture","page":"Home","title":"Neural network architecture","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The NN architecture is based upon that in the AlphaZero paper. The input to the neural network is a binary array mathbfz^0 in mathbbR^d. The paper states an input of mathbfz^0 in mathbbR^(8times8times(14h+7)) with a history length of h plies. Due to computational constraints we use only the current position h = 1 and discard the move counters to give an input of mathbbR^(8times8times17); 12 for the pieces, 4 for castling rights and 1 array for the colour of the side to move.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: ResNet block) A single ResNet block","category":"page"},{"location":"","page":"Home","title":"Home","text":"The NN is a Convolutional Neural Network (CNN), a NN that has seen great success in the fields of pattern recognition and computer vision. Specifically a Residual Neural Network (ResNet) is used. A single ResNet block is a two convolutional layers, each with a ReLu activation function, and batch norm between each layer. There is a skipped connection from the input of the two layers to the output, which is combined by addition and then a final ReLU.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Neural Network) The neural network architecture. The value head is skipped in this work. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output of the NN a 8times8times73 tensor, encoding the 73 possible moves for every square. The softmax function is applied to the output layer to convert the values to a probability distribution. The loss function is the cross-entropy loss, that standard loss function for multi-class classification problems. The network is trained on a selection of professional games with mini-batch gradient descent and the ADAM optimizer. The NN was built with the Julia Flux library and training used a single GPU (almost twice as fast as CPU). However, a single GPU is not nearly enough to properly train the full network and so the number of ResNet blocks was decreased from 20 to 6. Once the neural network was trained a severe bottleneck was encountered: predicting a move was very slow. Since MCTS requires the prediction of many moves, as the more rollouts the better the approximation of the value of the action, this proved an insurmountable problem (for now!).","category":"page"},{"location":"#Papers","page":"Home","title":"Papers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1] Browne, C.B. et al. (2012) “A survey of Monte Carlo Tree Search Methods,” IEEE Transactions on Computational Intelligence and AI in Games, 4(1), pp. 1–43. Available at: https://doi.org/10.1109/tciaig.2012.2186810.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]Silver, D. et al. (2016) “Mastering the game of go with deep neural networks and Tree Search,” Nature, 529(7587), pp. 484–489. Available at: https://doi.org/10.1038/nature16961.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]Silver, D. et al. (2018) “A general reinforcement learning algorithm that Masters Chess, Shogi, and go through self-play,” Science, 362(6419), pp. 1140–1144. Available at: https://doi.org/10.1126/science.aar6404. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"[4]McGrath, T. et al. (2022) “Acquisition of chess knowledge in AlphaZero,” Proceedings of the National Academy of Sciences, 119(47). Available at: https://doi.org/10.1073/pnas.2206625119. ","category":"page"}]
}
