<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · JuliaGo</title><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>JuliaGo</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#JuliaGo.jl-Documentation"><span>JuliaGo.jl Documentation</span></a></li><li><a class="tocitem" href="#Artificial-Intelligence-and-Games"><span>Artificial Intelligence and Games</span></a></li><li><a class="tocitem" href="#Monte-Carlo-Tree-Search"><span>Monte Carlo Tree Search</span></a></li><li><a class="tocitem" href="#Default-Policy:-training-a-neural-network-to-predict-moves"><span>Default Policy: training a neural network to predict moves</span></a></li></ul></li><li><a class="tocitem" href="Documentation/">Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/chris-haggard/JuliaGo.jl/blob/main/docs/src/index.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="JuliaGo.jl"><a class="docs-heading-anchor" href="#JuliaGo.jl">JuliaGo.jl</a><a id="JuliaGo.jl-1"></a><a class="docs-heading-anchor-permalink" href="#JuliaGo.jl" title="Permalink"></a></h1><p>An Julia chess engine based on MCTS and neural networks, similar to AlphaGo. This is a hobby project to further my understanding of artificial intelligence in the context of games and the Julia programming language. This began as an attempt to implement the full <a href="https://www.nature.com/articles/nature16961">AlphaGo paper</a>. This combines MCTS, neural networks and reinforcement learning. I only tackle the first two due to resource constraints (lack of GPUs) and time!</p><ul><li><a href="#JuliaGo.jl">JuliaGo.jl</a></li><li class="no-marker"><ul><li><a href="#JuliaGo.jl-Documentation">JuliaGo.jl Documentation</a></li><li><a href="#Artificial-Intelligence-and-Games">Artificial Intelligence and Games</a></li><li><a href="#Monte-Carlo-Tree-Search">Monte Carlo Tree Search</a></li><li><a href="#Default-Policy:-training-a-neural-network-to-predict-moves">Default Policy: training a neural network to predict moves</a></li></ul></li></ul><h2 id="JuliaGo.jl-Documentation"><a class="docs-heading-anchor" href="#JuliaGo.jl-Documentation">JuliaGo.jl Documentation</a><a id="JuliaGo.jl-Documentation-1"></a><a class="docs-heading-anchor-permalink" href="#JuliaGo.jl-Documentation" title="Permalink"></a></h2><p>Get started with the <a href="Documentation/#Documentation">Documentation</a>.</p><h2 id="Artificial-Intelligence-and-Games"><a class="docs-heading-anchor" href="#Artificial-Intelligence-and-Games">Artificial Intelligence and Games</a><a id="Artificial-Intelligence-and-Games-1"></a><a class="docs-heading-anchor-permalink" href="#Artificial-Intelligence-and-Games" title="Permalink"></a></h2><p>Two player games, such as Chess and Go, are often used as a testing ground for artificial intelligence algorithms. They provide a well-bounded problem, with obvious rules and a clear winner. Chess is a two-player zero-sum (an advantage for one side is an equivalent disadvantage for the other) perfect-information game and is the target of this work. A game of chess can be represented as a tree, referred to as the &#39;game-tree&#39;. This game-tree consists of nodes representing states in the game. The game-tree in chess has a branching factor, the average number of child nodes per parent, of 31-35. In the Go the branching factor is 200-250. The number of number of nodes grows exponentially, rendering a brute force approach impossible and so we turn to artificial intelligence algorithms.</p><p><img src="images/go_branching.png" alt="Go game-tree"/> <em>The Go game-tree</em></p><h3 id="Computer-Chess"><a class="docs-heading-anchor" href="#Computer-Chess">Computer Chess</a><a id="Computer-Chess-1"></a><a class="docs-heading-anchor-permalink" href="#Computer-Chess" title="Permalink"></a></h3><p>For a given chess board <span>$S_0$</span>, the aim of any chess engine (or human), is to find the optimal action <span>$A_i$</span> from a set of actions <span>$A_i \in \{A_1,\ldots,A_N\}$</span> that result in the next possible boards (states) <span>$\{S_1, \ldots, S_N\}$</span>. The optimal action is that which results, after many such actions, in a win for the player. The optimal value function <span>$\nu^*(s)$</span> is strictly 0 for non-terminal states and, in terminal states, <span>$[+1, 0, -1]$</span> for a win, draw and loss respectively. A brute-force approach would require recursively finding <span>$\nu^*(s)$</span> for every action/state in a game-tree.</p><p>The standard algorithm for computer chess in minimax. In minimax the game-tree is recursively explored by depth-first search (DFS) until a given predecided depth. At each state the action is taken to minimise the opponent’s maximum reward. </p><p class="math-container">\[\nu_i = \max_{a_i} \min_{a_{-i}}(\nu_i(a_i, a_{-i}))\]</p><p>where <span>$i \rightarrow -i$</span> indicates the switching of players. At the predecided maximum depth, which is very unlikely to be a terminal state, the remaining sub-tree is discarded and instead <span>$\nu^*(s)$</span> is approximated, <span>$\nu(s) \approx \nu^*(s)$</span>. <span>$\nu(s)$</span>, often called a static evaluation function, assigns a score to the state. This score reflects the likely outcome from the state and is necessarily heuristic, a function of the number of pieces and position, as a chess game is only decided by it&#39;s value in the terminal state (checkmate or draw). Good static evaluation functions are complex to create. The breadth of the game-tree is typically pruned with <span>$\alpha-\beta$</span> pruning.</p><p><img src="images/chess_minimax.webp" alt="Chess minimax"/> <em>Minimax algorithm for chess. At each state the action is selected that minimises the opponents maximum score. Only the terminal nodes are passed to the static evaluation function.</em></p><h2 id="Monte-Carlo-Tree-Search"><a class="docs-heading-anchor" href="#Monte-Carlo-Tree-Search">Monte Carlo Tree Search</a><a id="Monte-Carlo-Tree-Search-1"></a><a class="docs-heading-anchor-permalink" href="#Monte-Carlo-Tree-Search" title="Permalink"></a></h2><p>Monte Carlo Tree Search (MCTS) offers an alternative methodology to an evaluation function. Instead of terminating the search and evaluating the position heuristically, MCTS evaluates the actions available from a position by conducting rollouts - searches to terminal without any branching. Taking the average of many rollouts provides an approximation to the value of the given action. This is formulated as (following Browne et al.<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup>)</p><p class="math-container">\[Q(s, a) = \frac{1}{N(s,a)}\sum_{i=1}^{N(s)}\mathcal{I}_i(s,a)z_i\]</p><p>where</p><p class="math-container">\[\begin{aligned}
&amp;Q(s, a) \text{ - the value of taking action $a$ from state $s$}\\
&amp;N(s, a) \text{ - the number of times $a$ is selected from $s$}\\
&amp;N(s) \text{ - the total number of simulations from $s$}\\
&amp;z_i \text{ - the result of the $i$-th playout: [+1, 0, -1]}\\
&amp;\mathcal{I}_i(s, a) \text{ - is 1 if $a$ was selected from $s$ on the $i$-th playout.}
\end{aligned}\]</p><p>These values that are assigned to a specific action are then used to guide the search to best-first strategy. An asymmetric game-tree is progressively built - at each iteration the tree is guided by the previous search. The value associate with the most promising moves should become more accurate as the tree is built as they are preferentially searched in further depth as the tree grows. The key to MCTS is finding the balance between exploitation and exploration: exploiting the current most promising moves and exploring those that currently look less promising but could turn out to be more optimal given further exploration.</p><h3 id="MCTS-algorithm"><a class="docs-heading-anchor" href="#MCTS-algorithm">MCTS algorithm</a><a id="MCTS-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#MCTS-algorithm" title="Permalink"></a></h3><p><img src="images/MCTS_algo.png" alt="MCTS algorithm"/></p><p>The MCTS algorithm consists of 4 steps:</p><ul><li><em>Selection</em>: from <span>$S$</span> a selection policy that biases the search is recursively applied to child nodes until a non-fully expanded node is reached.</li><li><em>Expansion</em>: a child node is added to the tree.</li><li><em>Simulation</em>: a number of rollouts are played according to a default policy (a policy of selecting the next action) until a terminal state is reached</li><li><em>Backpropagation</em>: the results of the rollouts are propagated up the entire tree.</li></ul><p>The MCTS can be terminated at anytime and <span>$a$</span> is selected according to a policy.</p><h3 id="Biasing-the-search:-the-selection-policy"><a class="docs-heading-anchor" href="#Biasing-the-search:-the-selection-policy">Biasing the search: the selection policy</a><a id="Biasing-the-search:-the-selection-policy-1"></a><a class="docs-heading-anchor-permalink" href="#Biasing-the-search:-the-selection-policy" title="Permalink"></a></h3><p>The selection stage of the MCTS relies on a selection policy. The simplest policy would be to select child nodes from a uniform distribution. A more efficient method would bias the selection towards more promising actions. However, at the same time as exploiting promising actions, the selection policy must not totally discount less promising actions as these might be more favourable if the tree was explored in more depth. This is the key to <em>exploration-exploitation</em>. The most common selection policy is the Upper Confidence Bound for Trees (UCT):</p><p class="math-container">\[\mathrm{UCT} = \frac{w_i}{n_i} + C\sqrt{\frac{\ln N_i}{n_i}}\]</p><p>where</p><p class="math-container">\[\begin{aligned}
&amp;w_i \text{ - is the number of wins associated with taking the action that results in this node i.e. the number of wins from the parent&#39;s perspecive.}\\
&amp;n_i \text{ - is the number of simulations from this node.}\\
&amp;C = \sqrt{2} \text{ - is a coefficient controlling the balance between exploration and exploitation.}\\
&amp;N_i \text{ - is the number of simulations of the parent node.}
\end{aligned}\]</p><p>The child node with the maximum value of UCT is selected. The first term in UCT encourages the exploitation of promising actions and the second term biases towards less explored nodes. The selection policy results in asymmetric tree growth.</p><p><img src="images/asymmetric_tree.png" alt="Asymmetric tree growth"/> <em>The game-tree grows asymmetrically due to the selection policy.</em></p><h3 id="Selecting-an-action"><a class="docs-heading-anchor" href="#Selecting-an-action">Selecting an action</a><a id="Selecting-an-action-1"></a><a class="docs-heading-anchor-permalink" href="#Selecting-an-action" title="Permalink"></a></h3><p>At the end of MCTS, we are left with a set of actions <span>$\{A_1,\ldots,A_N\}$</span> each with an associated value <span>$Q$</span>. The standard method for selecting an action is the <em>robust child</em> method. This involves selecting <span>$a_i$</span> associated with creating <span>$s_i$</span> that has the highest number of visits (not the highest score i.e wins).</p><h2 id="Default-Policy:-training-a-neural-network-to-predict-moves"><a class="docs-heading-anchor" href="#Default-Policy:-training-a-neural-network-to-predict-moves">Default Policy: training a neural network to predict moves</a><a id="Default-Policy:-training-a-neural-network-to-predict-moves-1"></a><a class="docs-heading-anchor-permalink" href="#Default-Policy:-training-a-neural-network-to-predict-moves" title="Permalink"></a></h2><p>MCTS relies on a default policy to select moves in the rollouts. Randomly selecting moves would be a poor policy; it would require a great many rollouts to converge the value of the action. Instead we require a policy that selects the optimal action. This is impossible; if we knew the optimal action we would not be conducting MCTS. We approximate the optimal action with the action that a professional chess player would play. To extract this we need come supervised machine learning, for this we use a neural network (NN). The data set is a set of professional chess games. The input to the NN is the state and the output is the a probability distribution of legal moves.</p><h3 id="Neural-network-architecture"><a class="docs-heading-anchor" href="#Neural-network-architecture">Neural network architecture</a><a id="Neural-network-architecture-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-architecture" title="Permalink"></a></h3><p>The NN architecture is based upon that in the <a href="https://www.pnas.org/doi/10.1073/pnas.2206625119">AlphaZero paper</a>. The input to the neural network is a binary array <span>$\mathbf{z}^0 \in \mathbb{R}^d$</span>. The paper states an input of <span>$\mathbf{z}^0 \in \mathbb{R}^{(8\times8\times(14h+7))}$</span> with a history length of <span>$h$</span> plies. Due to computational constraints we use only the current position <span>$h = 1$</span> and discard the move counters to give an input of <span>$\mathbb{R}^{(8\times8\times17)}$</span>; 12 for the pieces, 4 for castling rights and 1 array for the colour of the side to move.</p><p><img src="images/resnetblock.png" alt="ResNet block"/> <em>A single ResNet block</em></p><p>The NN is a Convolutional Neural Network (CNN), a NN that has seen great success in the fields of pattern recognition and computer vision. Specifically a Residual Neural Network (ResNet) is used. A single ResNet block is a two convolutional layers, each with a ReLu activation function, and batch norm between each layer. There is a skipped connection from the input of the two layers to the output, which is combined by addition and then a final ReLU.</p><p><img src="images/nn.png" alt="Neural Network"/> <em>The neural network architecture. The value head is skipped in this work.</em> </p><p>The output of the NN a <span>$8\times8\times73$</span> tensor, encoding the 73 possible moves for every square. The softmax function is applied to the output layer to convert the values to a probability distribution. The loss function is the cross-entropy loss, that standard loss function for multi-class classification problems. The network is trained on a selection of professional games with mini-batch gradient descent and the ADAM optimizer. The NN was built with the Julia Flux library and training used a single GPU (almost twice as fast as CPU). However, a single GPU is not nearly enough to properly train the full network and so the number of ResNet blocks was decreased from 20 to 6. Once the neural network was trained a severe bottleneck was encountered: predicting a move was very slow. Since MCTS requires the prediction of many moves, as the more rollouts the better the approximation of the value of the action, this proved an insurmountable problem (for now!).</p><p><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> <a href="https://ieeexplore.ieee.org/document/6145622">Browne, C.B. et al. (2012) “A survey of Monte Carlo Tree Search Methods,” IEEE Transactions on Computational Intelligence and AI in Games, 4(1), pp. 1–43. Available at: https://doi.org/10.1109/tciaig.2012.2186810.</a></p></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="Documentation/">Documentation »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Saturday 14 January 2023 22:56">Saturday 14 January 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
